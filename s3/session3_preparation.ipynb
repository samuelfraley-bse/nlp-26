{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1589caf1",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5354d",
   "metadata": {},
   "source": [
    "## Introduction to Text Mining and Natural Language Processing\n",
    "\n",
    "Before we start we need to install a few more things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee82b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830f9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hannesfelixmuller/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hannesfelixmuller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hannesfelixmuller/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afc81b",
   "metadata": {},
   "source": [
    "## Session 3: Text Mining\n",
    "\n",
    "This session will be about pre-processing text. This is a very important part of text analysis and we will spend some time doing things by \"hand\" that we later use packages for. We will do this to understand why we are pre-processing in a particular way and how pre-processing can be used to explore the corpus.\n",
    "\n",
    "We will go through all steps:\n",
    "- **1) Tokenization**\n",
    "\n",
    "- **2) Normalization**\n",
    "  - A) Lemmatizing\n",
    "  - B) Unifying text (e.g., removing punctuation, converting to lower case)\n",
    "  - C) Removing stopwords\n",
    "  - D) Stemming\n",
    "\n",
    "- **3) Counting (N-grams) -> Document-Term Matrix (Vectorization)**\n",
    "\n",
    "This is not the order you typically see on webpages talking about pre-processing but I think it is the actual order in which you approach things chronologically. Lemmatizing is the really odd one in the order. It is also the most complex and the least well-packaged. So worth spending a little time on.\n",
    "\n",
    "We will ignore N-grams today when making counts of unigrams just to keep things as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88618dba",
   "metadata": {},
   "source": [
    "# 1) Tokenizing\n",
    "\n",
    "Tokenization is the process of breaking down a string of text into smaller pieces, called tokens. In the context of text mining and natural language processing (NLP), tokenization refers to the process of splitting text into words, phrases, symbols, or other meaningful elements, known as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59308326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#for later\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26136d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'president', 'of', 'the', 'United', 'States', '(', 'US', ')', 'is', 'Joe', 'Biden', '.']\n",
      "['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country', '.']\n",
      "['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden', '?']\n",
      "['A', 'really', 'well-known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh', '.']\n",
      "['The', 'leaves', 'were', 'left', 'untouched', ',', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define some example sentences\n",
    "sentence1 = \"The president of the United States (US) is Joe Biden.\"\n",
    "sentence2 = \"Joe Biden the president wants us in a united country.\"\n",
    "sentence3 = \"Did a known artist paint portraits of Joe Biden?\"\n",
    "sentence4 = \"A really well-known portrait artist is Vincent van Gogh.\"\n",
    "sentence5 = \"The leaves were left untouched, and the bats were unable to bat away the flies.\"\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokens1 = nltk.word_tokenize(sentence1)\n",
    "tokens2 = nltk.word_tokenize(sentence2)\n",
    "tokens3 = nltk.word_tokenize(sentence3)\n",
    "tokens4 = nltk.word_tokenize(sentence4)\n",
    "tokens5 = nltk.word_tokenize(sentence5)\n",
    "\n",
    "# Tag the tokens with their part of speech\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)\n",
    "print(tokens4)\n",
    "print(tokens5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d131e",
   "metadata": {},
   "source": [
    "# 2) Normalizing\n",
    "## A) Lemmatizer\n",
    "\n",
    "Lemmatizer actually uses grammar - we can tell the lemmatizer through the position of the word where the word stands. But for this we need to do leave the sentence unprocessed.\n",
    "\n",
    "Alternatively, we can pass sentences and the lemmatizer figures it out by itself. A first example of what the lemmatizer can figure out by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1055ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7fd1328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'president', 'of', 'the', 'United', 'States', '(', 'US', ')', 'is', 'Joe', 'Biden', '.']\n",
      "The president of the United States ( US ) is Joe Biden .\n",
      "['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country', '.']\n",
      "Joe Biden the president want u in a united country .\n",
      "['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden', '?']\n",
      "Did a known artist paint portrait of Joe Biden ?\n",
      "['A', 'really', 'well-known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh', '.']\n",
      "A really well-known portrait artist is Vincent van Gogh .\n",
      "['The', 'leaves', 'were', 'left', 'untouched', ',', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies', '.']\n",
      "The leaf were left untouched , and the bat were unable to bat away the fly .\n"
     ]
    }
   ],
   "source": [
    "sentences=[tokens1,tokens2,tokens3,tokens4,tokens5]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    lemmatized_sentence = []\n",
    "    for word in sentence:\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
    "    print(\" \".join(lemmatized_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b45889",
   "metadata": {},
   "source": [
    "This has very little impact on the text. However, lemmatizing becomes much more powerful if use the function of words in a text. We will now switch to the package spacy quickly because it has an immediate implementation of positioning.\n",
    "\n",
    "Below, spacy is called as sp. We pass it the sentence and it generates an object saved under sp_text. Spacy is very powerful with lots of functions. We will only use the word positioning which is stored under word.pos_. For more read this here: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beef1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the United States (US) is Joe Biden.\n",
      "The DET\n",
      "president NOUN\n",
      "of ADP\n",
      "the DET\n",
      "United PROPN\n",
      "States PROPN\n",
      "( PUNCT\n",
      "US PROPN\n",
      ") PUNCT\n",
      "is AUX\n",
      "Joe PROPN\n",
      "Biden PROPN\n",
      ". PUNCT\n",
      "the president of the United States ( US ) be Joe Biden .\n",
      " \n",
      "Joe Biden the president wants us in a united country.\n",
      "Joe PROPN\n",
      "Biden PROPN\n",
      "the DET\n",
      "president PROPN\n",
      "wants VERB\n",
      "us PRON\n",
      "in ADP\n",
      "a DET\n",
      "united ADJ\n",
      "country NOUN\n",
      ". PUNCT\n",
      "Joe Biden the president want we in a united country .\n",
      " \n",
      "Did a known artist paint portraits of Joe Biden?\n",
      "Did VERB\n",
      "a DET\n",
      "known VERB\n",
      "artist NOUN\n",
      "paint NOUN\n",
      "portraits NOUN\n",
      "of ADP\n",
      "Joe PROPN\n",
      "Biden PROPN\n",
      "? PUNCT\n",
      "do a know artist paint portrait of Joe Biden ?\n",
      " \n",
      "A really well-known portrait artist is Vincent van Gogh.\n",
      "A DET\n",
      "really ADV\n",
      "well ADV\n",
      "- PUNCT\n",
      "known VERB\n",
      "portrait NOUN\n",
      "artist NOUN\n",
      "is AUX\n",
      "Vincent PROPN\n",
      "van PROPN\n",
      "Gogh PROPN\n",
      ". PUNCT\n",
      "a really well - know portrait artist be Vincent van Gogh .\n",
      " \n",
      "The leaves were left untouched, and the bats were unable to bat away the flies.\n",
      "The DET\n",
      "leaves NOUN\n",
      "were AUX\n",
      "left VERB\n",
      "untouched ADJ\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "the DET\n",
      "bats NOUN\n",
      "were AUX\n",
      "unable ADJ\n",
      "to PART\n",
      "bat VERB\n",
      "away ADV\n",
      "the DET\n",
      "flies NOUN\n",
      ". PUNCT\n",
      "the leaf be leave untouched , and the bat be unable to bat away the fly .\n",
      " \n"
     ]
    }
   ],
   "source": [
    "sentences=[sentence1,sentence2,sentence3,sentence4, sentence5]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    sp_text=sp(sentence)\n",
    "    lemmatized_sentence = []\n",
    "    for word in sp_text:\n",
    "        print(word, word.pos_)\n",
    "        lemmatized_sentence.append(word.lemma_)\n",
    "    print(\" \".join(lemmatized_sentence))\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a92b7b",
   "metadata": {},
   "source": [
    "## Remove punctuation and unify, e.g. convert to lower case\n",
    "\n",
    "We now start back from the tokenized text as if we did not decide to do lemmatizing. Obviously, this is not what you want to do if lemmatizing is part of your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3be17",
   "metadata": {},
   "source": [
    "### Tokenize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f277b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the United States (US) is Joe Biden.\n",
      "['The', 'president', 'of', 'the', 'United', 'States', '(US)', 'is', 'Joe', 'Biden.']\n",
      "Joe Biden the president wants us in a united country.\n",
      "['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country.']\n",
      "Did a known artist paint portraits of Joe Biden?\n",
      "['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden?']\n",
      "A really well-known portrait artist is Vincent van Gogh.\n",
      "['A', 'really', 'well-known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh.']\n",
      "The leaves were left untouched, and the bats were unable to bat away the flies.\n",
      "['The', 'leaves', 'were', 'left', 'untouched,', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    words = sentence.split()\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1c209",
   "metadata": {},
   "source": [
    "### B) Unify 1 - getting rid of non-words\n",
    "\n",
    "\n",
    "This regular expression below (re.split(r'\\W+', sentence)) splits the input sentence into words by using any sequence of non-word characters (such as spaces, punctuation, etc.) as delimiters. This is useful for preprocessing text because it effectively separates and extracts individual words, making the text easier to process, analyze, or manipulate for tasks like word frequency analysis, natural language processing, or text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91aff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the United States (US) is Joe Biden.\n",
      "['The', 'president', 'of', 'the', 'United', 'States', 'US', 'is', 'Joe', 'Biden', '']\n",
      "Joe Biden the president wants us in a united country.\n",
      "['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country', '']\n",
      "Did a known artist paint portraits of Joe Biden?\n",
      "['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden', '']\n",
      "A really well-known portrait artist is Vincent van Gogh.\n",
      "['A', 'really', 'well', 'known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh', '']\n",
      "The leaves were left untouched, and the bats were unable to bat away the flies.\n",
      "['The', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies', '']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    words = re.split(r'\\W+', sentence)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20be0ec",
   "metadata": {},
   "source": [
    "### B) Unify 2 - more elegant\n",
    "\n",
    "In the context below of re.sub(r'\\W+', '', word), the term \"sub\" stands for \"substitute\". It's a function in Python's regular expression (re) module that is used for substituting all occurrences of a specified pattern in a given string with another string. Here, the pattern \\W+ (which matches any sequence of non-word characters) is being replaced with an empty string '', effectively removing these characters from the input word. This substitution feature is a powerful tool in text processing for modifying and cleaning data.\n",
    "\n",
    "Why the part \"if strip(w)\" in the command [strip(w) for w in words if strip(w)]? Because it only allows elements in that are not empty. This can happen if several non-word characters follow each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2dc0934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'president', 'of', 'the', 'United', 'States', 'US', 'is', 'Joe', 'Biden']\n",
      "['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country']\n",
      "['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden']\n",
      "['A', 'really', 'wellknown', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh']\n",
      "['The', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies']\n"
     ]
    }
   ],
   "source": [
    "#let's write our own function to do this\n",
    "def strip(word):\n",
    "    mod_string = re.sub(r'\\W+', '', word)\n",
    "    return mod_string\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    stripped = [strip(w) for w in words if strip(w)]\n",
    "    print(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206078c",
   "metadata": {},
   "source": [
    "### B) Unify 3 - add lowercasing\n",
    "\n",
    "Lowercasing is super easy to integrate in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2a457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'president', 'of', 'the', 'united', 'states', 'us', 'is', 'joe', 'biden']\n",
      "['joe', 'biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country']\n",
      "['did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'joe', 'biden']\n",
      "['a', 'really', 'wellknown', 'portrait', 'artist', 'is', 'vincent', 'van', 'gogh']\n",
      "['the', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    lowered = [strip(w).lower() for w in words]\n",
    "    print(lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a74eb",
   "metadata": {},
   "source": [
    "### Do you see a problem?\n",
    "\n",
    "There is a problem with the code above. Depending on your application you don't like what it does. \n",
    "\n",
    "What does the code below do? (remove period to check)\n",
    "\n",
    "### B) Unify 4 - special lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1da24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'president', 'of', 'the', 'united', 'states', 'US', 'is', 'joe', 'biden']\n",
      "['joe', 'biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country']\n",
      "['did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'joe', 'biden']\n",
      "['a', 'really', 'wellknown', 'portrait', 'artist', 'is', 'vincent', 'van', 'gogh']\n",
      "['the', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies']\n"
     ]
    }
   ],
   "source": [
    "def abbr_or_lower(word):\n",
    "    if re.match('([A-Z]+[a-z]*){2,}', word):\n",
    "        return word\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "corpus=[]\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    lowered = [abbr_or_lower(strip(w)) for w in words]\n",
    "    print(lowered)\n",
    "    corpus.append(lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e2701",
   "metadata": {},
   "source": [
    "### A) +  B) but using spaCy's own abilities\n",
    "\n",
    "When you process text with spaCy (e.g., using doc = sp(sentence)), the pipeline includes several components—tokenization, part-of-speech (POS) tagging, morphological analysis, and (if enabled) named entity recognition. However, the lemmatizer itself primarily relies on the POS tags and morphological features, not directly on the recognized named entities.\n",
    "\n",
    "Lemmatization Behavior:\n",
    "For proper nouns (like “Joe” or “Biden”) and abbreviations (like “US”), the lemmatizer is designed to leave them unchanged because they typically don’t have “more basic” forms. In other words, the rule-based lemmatizer in spaCy is set up so that proper nouns and abbreviations remain as they are. Even though NER might have identified “Joe Biden” as a named entity, the lemmatizer isn’t altering the case or form based on that information; it’s simply using the token’s morphological attributes.\n",
    "\n",
    "Preserving Capitalization:\n",
    "The process of lemmatization does not force everything to lowercase. If a token’s lemma is determined to be the same as the original proper noun or abbreviation, it will remain in its original case. This is why “Joe Biden” remains capitalized and “US” stays as “US.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3900e1a-8a02-4720-8de5-d50ffd8a14a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'president', 'of', 'the', 'United', 'States', 'US', 'be', 'Joe', 'Biden']\n",
      "['Joe', 'Biden', 'the', 'president', 'want', 'we', 'in', 'a', 'united', 'country']\n",
      "['do', 'a', 'know', 'artist', 'paint', 'portrait', 'of', 'Joe', 'Biden']\n",
      "['a', 'really', 'well', 'know', 'portrait', 'artist', 'be', 'Vincent', 'van', 'Gogh']\n",
      "['the', 'leaf', 'be', 'leave', 'untouched', 'and', 'the', 'bat', 'be', 'unable', 'to', 'bat', 'away', 'the', 'fly']\n"
     ]
    }
   ],
   "source": [
    "corpus_pos_lemm = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Process the sentence with spaCy\n",
    "    doc = sp(sentence)\n",
    "    \n",
    "    # Extract the lemma for each token, filtering out punctuation and spaces.\n",
    "    processed_tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    \n",
    "    print(processed_tokens)\n",
    "    corpus_pos_lemm.append(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5adb",
   "metadata": {},
   "source": [
    "## C) Stopwords\n",
    "\n",
    "This is a very interesting concept. Stopwords are something we agree on are generally not meaningful to distinguish one document from another. \n",
    "\n",
    "Note: the idea of stopwords is closely linked to the bag-of-words model where grammar doesn't play a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a0b68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president', 'united', 'states', 'US', 'joe', 'biden']\n",
      "['joe', 'biden', 'president', 'wants', 'us', 'united', 'country']\n",
      "['known', 'artist', 'paint', 'portraits', 'joe', 'biden']\n",
      "['really', 'wellknown', 'portrait', 'artist', 'vincent', 'van', 'gogh']\n",
      "['leaves', 'left', 'untouched', 'bats', 'unable', 'bat', 'away', 'flies']\n"
     ]
    }
   ],
   "source": [
    "corpus_stop=[]\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    lowered = [abbr_or_lower(strip(w)) for w in words if abbr_or_lower(strip(w)) not in set(stopwords.words('english'))]\n",
    "    print(lowered)\n",
    "    corpus_stop.append(lowered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b088a1",
   "metadata": {},
   "source": [
    "## D) Stemming\n",
    "\n",
    "\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\"\n",
    "\n",
    "nltk allows you to choose between PorterStammer or LancasterStammer, PorterStemmer being the oldest one originally developed in 1979. LancasterStemmer was developed in 1990 and uses a more aggressive approach than Porter Stemming Algorithm. Then we have the SnowballStemmer.\n",
    "\n",
    "For Spanish we have the SnowballStemmer.\n",
    "\n",
    "Stemming is standard in text mining. But it does not always make sense. My experience is that it really depends on the application whether this is worth it. One important aspect is that sometimes stemming is quite brutal leading to very harsh cuts. Can be useful if dimensionality reduction is an important concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc710d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "#english and spanish language\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "spanishStemmer=SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "246e0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Lancaster Stemmer   Snowball Stemmer              \n",
      "friend              friend              friend              friend                        \n",
      "friendship          friendship          friend              friendship                    \n",
      "friends             friend              friend              friend                        \n",
      "friendships         friendship          friend              friendship                    \n",
      "stable              stabl               stabl               stabl                         \n",
      "stabilize           stabil              stabl               stabil                        \n",
      "stabilized          stabil              stabl               stabil                        \n",
      "misunderstanding    misunderstand       misunderstand       misunderstand                 \n",
      "footballers         footbal             footbal             footbal                       \n",
      "fighter             fighter             fight               fighter                       \n",
      "fought              fought              fought              fought                        \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stable\",\"stabilize\",\"stabilized\", \"misunderstanding\",\"footballers\", \"fighter\", \"fought\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:30}\".format(\"Word\",\"Porter Stemmer\",\"Lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:30}\".format(word,porter.stem(word),lancaster.stem(word), englishStemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ca7bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Snowball Stemmer    \n",
      "amigo               amig                \n",
      "amigos              amig                \n",
      "amigas              amig                \n",
      "correr              corr                \n",
      "corriente           corrient            \n",
      "incertidumbre       incertidumbr        \n",
      "luchadores          luchador            \n",
      "desempleo           desemple            \n",
      "fueron              fueron              \n"
     ]
    }
   ],
   "source": [
    "#A spanish list of words to be stemmed\n",
    "word_list = [\"amigo\", \"amigos\", \"amigas\", \"correr\",\"corriente\",\"incertidumbre\",\"luchadores\", \"desempleo\",\"fueron\"]\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}\".format(word,spanishStemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43224711",
   "metadata": {},
   "source": [
    "#### Bringing stemming back to the pipeline\n",
    "\n",
    "Note that the stemmer has a lower case maker but not a punctuation strip included. I therefore kicked out the abbr_or_lower() function but left the strip(w) in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f25ce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'presid', 'of', 'the', 'unit', 'state', 'us', 'is', 'joe', 'biden']\n",
      "['joe', 'biden', 'the', 'presid', 'want', 'us', 'in', 'a', 'unit', 'countri']\n",
      "['did', 'a', 'known', 'artist', 'paint', 'portrait', 'of', 'joe', 'biden']\n",
      "['a', 'realli', 'wellknown', 'portrait', 'artist', 'is', 'vincent', 'van', 'gogh']\n",
      "['the', 'leav', 'were', 'left', 'untouch', 'and', 'the', 'bat', 'were', 'unabl', 'to', 'bat', 'away', 'the', 'fli']\n"
     ]
    }
   ],
   "source": [
    "corpus_stem=[]\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    lowered = [englishStemmer.stem(strip(w)) for w in words]\n",
    "    print(lowered)\n",
    "    corpus_stem.append(lowered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109c36f",
   "metadata": {},
   "source": [
    "# 3) Vectorization\n",
    "\n",
    "Now comes the most important step in the whole course. We will now represent the article as a vector - through it's terms. We will for now do this is the simplest possible form without dropping anything and focusing on unigrams.\n",
    "\n",
    "Conceptually important here is the fact that we want to construct a dictionary of how positions in the vector (the index) relate to terms. But we can then forget about the meaning of the index and just treat the documents vectors of numbers. This is wonderful as it allows us to do all the cool stuff that we can do with vectors.\n",
    "\n",
    "The following code goes through the corpus that we made above. Let's try different ways of making the corpus and think about the vectors this generates.\n",
    "\n",
    "### Start with lowercased corpus to build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54aaa8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'president': 2, 'of': 3, 'united': 4, 'states': 5, 'US': 6, 'is': 7, 'joe': 8, 'biden': 9, 'wants': 10, 'us': 11, 'in': 12, 'a': 13, 'country': 14, 'did': 15, 'known': 16, 'artist': 17, 'paint': 18, 'portraits': 19, 'really': 20, 'wellknown': 21, 'portrait': 22, 'vincent': 23, 'van': 24, 'gogh': 25, 'leaves': 26, 'were': 27, 'left': 28, 'untouched': 29, 'and': 30, 'bats': 31, 'unable': 32, 'to': 33, 'bat': 34, 'away': 35, 'flies': 36}\n",
      " \n",
      "Total size of vocabulary is: 36\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "for doc in corpus:\n",
    "    for token in doc:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "\n",
    "vocab_lower=vocab\n",
    "vocab_lower_size = len(vocab)\n",
    "print(vocab_lower)\n",
    "print(\" \")\n",
    "print(\"Total size of vocabulary is:\", vocab_lower_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8b24c",
   "metadata": {},
   "source": [
    "### Stopword removed corpus\n",
    "\n",
    "Why do we lose dimensions compared to the lowercased version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fc5fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'president': 1, 'united': 2, 'states': 3, 'US': 4, 'joe': 5, 'biden': 6, 'wants': 7, 'us': 8, 'country': 9, 'known': 10, 'artist': 11, 'paint': 12, 'portraits': 13, 'really': 14, 'wellknown': 15, 'portrait': 16, 'vincent': 17, 'van': 18, 'gogh': 19, 'leaves': 20, 'left': 21, 'untouched': 22, 'bats': 23, 'unable': 24, 'bat': 25, 'away': 26, 'flies': 27}\n",
      " \n",
      "Total size of vocabulary with stopwords removed is: 27\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "for doc in corpus_stop:\n",
    "    for token in doc:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "        \n",
    "vocab_stop=vocab\n",
    "vocab_stop_size = len(vocab)\n",
    "print(vocab_stop)\n",
    "print(\" \")\n",
    "print(\"Total size of vocabulary with stopwords removed is:\", vocab_stop_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bda440",
   "metadata": {},
   "source": [
    "### Stemmed corpus\n",
    "\n",
    "Why do we lose dimensions compared to the lowercased version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "281ee802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'presid': 2, 'of': 3, 'unit': 4, 'state': 5, 'us': 6, 'is': 7, 'joe': 8, 'biden': 9, 'want': 10, 'in': 11, 'a': 12, 'countri': 13, 'did': 14, 'known': 15, 'artist': 16, 'paint': 17, 'portrait': 18, 'realli': 19, 'wellknown': 20, 'vincent': 21, 'van': 22, 'gogh': 23, 'leav': 24, 'were': 25, 'left': 26, 'untouch': 27, 'and': 28, 'bat': 29, 'unabl': 30, 'to': 31, 'away': 32, 'fli': 33}\n",
      " \n",
      "Total size of stemmed vocabulary: 33\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "for doc in corpus_stem:\n",
    "    for token in doc:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "        \n",
    "vocab_stem=vocab\n",
    "vocab_stem_size = len(vocab)\n",
    "print(vocab_stem)\n",
    "print(\" \")\n",
    "print(\"Total size of stemmed vocabulary:\", vocab_stem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afacb6",
   "metadata": {},
   "source": [
    "### Lemmatize then unified\n",
    "\n",
    "Why do we lose dimensions compared to the lowercased version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b80fa22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'president': 2, 'of': 3, 'United': 4, 'States': 5, 'US': 6, 'be': 7, 'Joe': 8, 'Biden': 9, 'want': 10, 'we': 11, 'in': 12, 'a': 13, 'united': 14, 'country': 15, 'do': 16, 'know': 17, 'artist': 18, 'paint': 19, 'portrait': 20, 'really': 21, 'well': 22, 'Vincent': 23, 'van': 24, 'Gogh': 25, 'leaf': 26, 'leave': 27, 'untouched': 28, 'and': 29, 'bat': 30, 'unable': 31, 'to': 32, 'away': 33, 'fly': 34}\n",
      " \n",
      "Total size of stemmed vocabulary: 34\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "for doc in corpus_pos_lemm:\n",
    "    for token in doc:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "        \n",
    "vocab_pos_lemm=vocab\n",
    "vocab_pos_lemm_size = len(vocab)\n",
    "print(vocab_pos_lemm)\n",
    "print(\" \")\n",
    "print(\"Total size of stemmed vocabulary:\", vocab_pos_lemm_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb13c67",
   "metadata": {},
   "source": [
    "### Moving on. Let's vectorize our documents using the lowercased vocabulary.\n",
    "\n",
    "Let's run a count of terms in the vocab_lower on the first lowercased sentense. \n",
    "\n",
    "Make sure you understand what the following cell does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70d19355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 3\n",
      "president 0\n",
      "of 0\n",
      "united 0\n",
      "states 0\n",
      "US 0\n",
      "is 0\n",
      "joe 0\n",
      "biden 0\n",
      "wants 0\n",
      "us 0\n",
      "in 0\n",
      "a 0\n",
      "country 0\n",
      "did 0\n",
      "known 0\n",
      "artist 0\n",
      "paint 0\n",
      "portraits 0\n",
      "really 0\n",
      "wellknown 0\n",
      "portrait 0\n",
      "vincent 0\n",
      "van 0\n",
      "gogh 0\n",
      "leaves 1\n",
      "were 2\n",
      "left 1\n",
      "untouched 1\n",
      "and 1\n",
      "bats 1\n",
      "unable 1\n",
      "to 1\n",
      "bat 1\n",
      "away 1\n",
      "flies 1\n"
     ]
    }
   ],
   "source": [
    "for w in vocab_lower:\n",
    "    print(w, corpus[4].count(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968eadd2",
   "metadata": {},
   "source": [
    "We will write a function that uses this. Make sure you understand what the following thing does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21d80b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens, vocab):\n",
    "    vector=[]\n",
    "    for w in vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed8ab2",
   "metadata": {},
   "source": [
    "Now we apply this to the entire lowercased corpus, document by document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "150990d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "for doc in corpus:\n",
    "    vectors.append(vectorize(doc, vocab_lower))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907a256",
   "metadata": {},
   "source": [
    "Let's visualize this collection of vectors that we have generated from the documents making counts of terms in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "940e3642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  26  27  28  29  30  31  32  \\\n",
       "0   2   1   1   1   1   1   1   1   1   0  ...   0   0   0   0   0   0   0   \n",
       "1   1   1   0   1   0   0   0   1   1   1  ...   0   0   0   0   0   0   0   \n",
       "2   0   0   1   0   0   0   0   1   1   0  ...   0   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "4   3   0   0   0   0   0   0   0   0   0  ...   2   1   1   1   1   1   1   \n",
       "\n",
       "   33  34  35  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   1   1   1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a pandas dataframe\n",
    "df = pd.DataFrame(vectors)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4aa2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix has size: (5, 37)\n",
      "The column names of this matrix are:\n",
      "['Biden' 'Did' 'Gogh' 'Joe' 'States' 'The' 'US' 'United' 'Vincent' 'and'\n",
      " 'artist' 'away' 'bat' 'bats' 'country' 'flies' 'in' 'is' 'known' 'leaves'\n",
      " 'left' 'of' 'paint' 'portrait' 'portraits' 'president' 'really' 'the'\n",
      " 'to' 'unable' 'united' 'untouched' 'us' 'van' 'wants' 'well' 'were']\n"
     ]
    }
   ],
   "source": [
    "#just for fun lets do the same with the countvectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,1), lowercase=False)\n",
    "cv.fit(sentences)\n",
    "\n",
    "\n",
    "\n",
    "vectorized_text=cv.transform(sentences)\n",
    "vectorized_text=vectorized_text.todense()\n",
    "print(\"matrix has size:\", vectorized_text.shape)\n",
    "\n",
    "print(\"The column names of this matrix are:\")\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98ce1",
   "metadata": {},
   "source": [
    "The above is a huge step. It's the first time you see the kind of animal we will be dealing with the rest of the course. This is a mathematical vector representation of the corpus! This is how machines see language. \n",
    "\n",
    "## How is this animal called?\n",
    "\n",
    "The name of this matrix has been mentioned above and in the lecture.\n",
    "\n",
    "This is a key item we will come back to again and again - it is called the..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3214f",
   "metadata": {},
   "source": [
    "### What are the correct column names here?\n",
    "Think before removing the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ef6e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-db6bbb118ee0>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-db6bbb118ee0>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    .\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df.columns = vocab_lower.keys()\n",
    "df\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426cb74",
   "metadata": {},
   "source": [
    "Let's have some short fun with the matrix - we will come back to this but this is just to give you an idea how cool this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b9365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>united</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wants</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>did</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>known</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artist</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paint</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portraits</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wellknown</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portrait</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vincent</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>van</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gogh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leaves</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>untouched</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bats</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unable</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flies</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1  2  3  4\n",
       "the        2  1  0  0  3\n",
       "president  1  1  0  0  0\n",
       "of         1  0  1  0  0\n",
       "united     1  1  0  0  0\n",
       "states     1  0  0  0  0\n",
       "US         1  0  0  0  0\n",
       "is         1  0  0  1  0\n",
       "joe        1  1  1  0  0\n",
       "biden      1  1  1  0  0\n",
       "wants      0  1  0  0  0\n",
       "us         0  1  0  0  0\n",
       "in         0  1  0  0  0\n",
       "a          0  1  1  1  0\n",
       "country    0  1  0  0  0\n",
       "did        0  0  1  0  0\n",
       "known      0  0  1  0  0\n",
       "artist     0  0  1  1  0\n",
       "paint      0  0  1  0  0\n",
       "portraits  0  0  1  0  0\n",
       "really     0  0  0  1  0\n",
       "wellknown  0  0  0  1  0\n",
       "portrait   0  0  0  1  0\n",
       "vincent    0  0  0  1  0\n",
       "van        0  0  0  1  0\n",
       "gogh       0  0  0  1  0\n",
       "leaves     0  0  0  0  1\n",
       "were       0  0  0  0  2\n",
       "left       0  0  0  0  1\n",
       "untouched  0  0  0  0  1\n",
       "and        0  0  0  0  1\n",
       "bats       0  0  0  0  1\n",
       "unable     0  0  0  0  1\n",
       "to         0  0  0  0  1\n",
       "bat        0  0  0  0  1\n",
       "away       0  0  0  0  1\n",
       "flies      0  0  0  0  1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.transpose()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e421d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the United States (US) is Joe Biden.\n",
      "Joe Biden the president wants us in a united country.\n",
      "Did a known artist paint portraits of Joe Biden?\n",
      "A really well-known portrait artist is Vincent van Gogh.\n",
      "The leaves were left untouched, and the bats were unable to bat away the flies.\n",
      " \n",
      "Difference of sentence 1 to sentence 2 is 10\n",
      "Difference of sentence 2 to sentence 2 is 0\n",
      "Difference of sentence 3 to sentence 2 is 13\n",
      "Difference of sentence 4 to sentence 2 is 17\n",
      "Difference of sentence 5 to sentence 2 is 23\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "print(\" \")\n",
    "for i in range(0,5):\n",
    "    result=abs(df[1]-df[i])\n",
    "    print(\"Difference of sentence\",i+1, \"to sentence 2 is\", result.sum(axis=0))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f773388",
   "metadata": {},
   "source": [
    "# WOW THIS IS AMAZING - YOUR LIFE HAS CHANGED\n",
    "(a tiny little bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d71c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining-dsdm (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
