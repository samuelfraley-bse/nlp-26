{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Mining and Natural Language Processing\n",
    "\n",
    "\n",
    "## Session 4: Vectormath\n",
    "\n",
    "This notebook goes deeper into different tf-idf weightings. As a preamble we will go into understanding the functional forms of tf-idf formulas with the sklearn package and the classic formula. We will see that a critical component for the sklearn package is the sublinear_tf option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw count (x)   | Classic(df=50)  sklearn(df=50)  |  Classic(df=100)  sklearn(df=100)\n",
      "---------------+--------------------------------+------------------------------------\n",
      "             1 |         2.9957          3.9957  |          2.3026          3.3026\n",
      "             2 |         5.0722          6.7654  |          3.8986          5.5918\n",
      "             5 |         7.8172         10.4266  |          6.0085          8.6179\n",
      "            10 |         9.8937         13.1962  |          7.6045         10.9071\n",
      "            20 |        11.9701         15.9659  |          9.2005         13.1962\n",
      "     \n",
      "Now with sublinear_tf=False:\n",
      "             1 |         2.9957          3.9957  |          2.3026          3.3026\n",
      "             2 |         5.0722          7.9915  |          3.8986          6.6052\n",
      "             5 |         7.8172         19.9787  |          6.0085         16.5129\n",
      "            10 |         9.8937         39.9573  |          7.6045         33.0259\n",
      "            20 |        11.9701         79.9146  |          9.2005         66.0517\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def classic_tfidf(x, df, D):\n",
    "    \"\"\"\n",
    "    x      = raw count in the document\n",
    "    df     = number of documents containing the term\n",
    "    D      = total number of documents\n",
    "    Returns the \"classic\" textbook tf-idf\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        return 0.0\n",
    "    # log base can be e or 10 - typically it doesn't matter as it only changes scaling\n",
    "    return (1.0 + math.log(x)) * math.log(D / df)\n",
    "\n",
    "def sklearn_like_tfidf(x, df, D, sublinear_tf=False, smooth_idf=True):\n",
    "    \"\"\"\n",
    "    Rough replication of how scikit-learn does its weighting by default.\n",
    "    \"\"\"\n",
    "    # 1) TF part\n",
    "    if sublinear_tf and x > 0:\n",
    "        tf = 1.0 + math.log(x)\n",
    "    else:\n",
    "        tf = float(x)\n",
    "    \n",
    "    # 2) IDF part\n",
    "    #  - if smooth_idf=True, formula is log((1 + D)/(1 + df)) + 1\n",
    "    #  - if smooth_idf=False, formula is log(D/df) + 1\n",
    "    if smooth_idf:\n",
    "        idf = math.log((1.0 + D)/(1.0 + df)) + 1.0\n",
    "    else:\n",
    "        idf = math.log(D/df) + 1.0\n",
    "    \n",
    "    return tf * idf\n",
    "\n",
    "# --- Compare the two for different df values in a small example\n",
    "\n",
    "D           = 1000          # total documents\n",
    "df_values   = [50, 100]     # two different df values\n",
    "x_values    = [1, 2, 5, 10, 20]  # raw counts to examine\n",
    "\n",
    "print(\"Raw count (x)   | Classic(df=50)  sklearn(df=50)  |  Classic(df=100)  sklearn(df=100)\")\n",
    "print(\"---------------+--------------------------------+------------------------------------\")\n",
    "\n",
    "for x in x_values:\n",
    "    # For df=50\n",
    "    c50 = classic_tfidf(x, df_values[0], D)\n",
    "    s50 = sklearn_like_tfidf(x, df_values[0], D, sublinear_tf=True, smooth_idf=False)\n",
    "    \n",
    "    # For df=100\n",
    "    c100 = classic_tfidf(x, df_values[1], D)\n",
    "    s100 = sklearn_like_tfidf(x, df_values[1], D, sublinear_tf=True, smooth_idf=False)\n",
    "    \n",
    "    print(f\"{x:14d} | {c50:14.4f}  {s50:14.4f}  |  {c100:14.4f}  {s100:14.4f}\")\n",
    "\n",
    "print(\"     \")\n",
    "print(\"Now with sublinear_tf=False:\")\n",
    "\n",
    "for x in x_values:\n",
    "    # For df=50\n",
    "    c50 = classic_tfidf(x, df_values[0], D)\n",
    "    s50 = sklearn_like_tfidf(x, df_values[0], D, sublinear_tf=False, smooth_idf=False)\n",
    "    \n",
    "    # For df=100\n",
    "    c100 = classic_tfidf(x, df_values[1], D)\n",
    "    s100 = sklearn_like_tfidf(x, df_values[1], D, sublinear_tf=False, smooth_idf=False)\n",
    "    \n",
    "    print(f\"{x:14d} | {c50:14.4f}  {s50:14.4f}  |  {c100:14.4f}  {s100:14.4f}\")\n",
    "\n",
    "# Note: smoothing makes only small difference overall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding tf-idf in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document term matrix has size (4, 26)\n",
      "['artist' 'countries' 'country' 'did' 'donald' 'gogh' 'in' 'is' 'known'\n",
      " 'of' 'other' 'paint' 'portrait' 'portraits' 'president' 'really' 'states'\n",
      " 'the' 'to' 'trump' 'united' 'us' 'van' 'vincent' 'wants' 'well']\n"
     ]
    }
   ],
   "source": [
    "# Define some example sentences\n",
    "sentence1 = \"The president of the United States (US) is president Donald Trump.\"\n",
    "sentence2 = \"Donald Trump the president wants us in a united country to trump other countries.\"\n",
    "sentence3 = \"Did a known artist paint portraits of Donald Trump?\"\n",
    "sentence4 = \"A really well-known portrait artist is Vincent van Gogh.\"\n",
    "\n",
    "corpus=[sentence1,sentence2,sentence3,sentence4]\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,1))\n",
    "cv.fit(corpus)\n",
    "vectorized_text=cv.transform(corpus)\n",
    "vectorized_text=vectorized_text.todense()\n",
    "print(\"document term matrix has size\", vectorized_text.shape)\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 1 0 0 0 0 2 0 1 2 0 1 1 1 0 0 0 0]]\n",
      "[[0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 2 1 1 0 0 1 0]]\n",
      "[[1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0]]\n",
      "[[1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "for document in vectorized_text:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artist' 'countries' 'country' 'did' 'donald' 'gogh' 'in' 'is' 'known'\n",
      " 'of' 'other' 'paint' 'portrait' 'portraits' 'president' 'really' 'states'\n",
      " 'the' 'to' 'trump' 'united' 'us' 'van' 'vincent' 'wants' 'well']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document term matrix has size (4, 26)\n",
      "['artist' 'countries' 'country' 'did' 'donald' 'gogh' 'in' 'is' 'known'\n",
      " 'of' 'other' 'paint' 'portrait' 'portraits' 'president' 'really' 'states'\n",
      " 'the' 'to' 'trump' 'united' 'us' 'van' 'vincent' 'wants' 'well']\n"
     ]
    }
   ],
   "source": [
    "cv = TfidfVectorizer(ngram_range = (1,1), norm=None, sublinear_tf=False)\n",
    "cv.fit(corpus)\n",
    "vectorized_text=cv.transform(corpus)\n",
    "vectorized_text=vectorized_text.todense()\n",
    "print(\"document term matrix has size\", vectorized_text.shape)\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.    1.223 0.    0.    1.511 0.    1.511 0.    0.\n",
      " 0.    0.    3.022 0.    1.916 3.022 0.    1.223 1.511 1.511 0.    0.\n",
      " 0.    0.   ]\n",
      "[0.    1.916 1.916 0.    1.223 0.    1.916 0.    0.    0.    1.916 0.\n",
      " 0.    0.    1.511 0.    0.    1.511 1.916 2.446 1.511 1.511 0.    0.\n",
      " 1.916 0.   ]\n",
      "[1.511 0.    0.    1.916 1.223 0.    0.    0.    1.511 1.511 0.    1.916\n",
      " 0.    1.916 0.    0.    0.    0.    0.    1.223 0.    0.    0.    0.\n",
      " 0.    0.   ]\n",
      "[1.511 0.    0.    0.    0.    1.916 0.    1.511 1.511 0.    0.    0.\n",
      " 1.916 0.    0.    1.916 0.    0.    0.    0.    0.    0.    1.916 1.916\n",
      " 0.    1.916]\n"
     ]
    }
   ],
   "source": [
    "vectorized_text=vectorized_text.round(decimals=3, out=None)\n",
    "for document in vectorized_text:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "The formula that is used to compute the tf-idf for a term v of a document d in a document set is tf-idf(v, d) = tf(v, d) * idf(v), and the idf is computed as idf(v) = log [ D / df(v) ] + 1 (if smooth_idf=False), where D is the total number of documents in the document set and df(v) is the document frequency of v; the document frequency is the number of documents in the document set that contain the term t. The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(v) = log [ D / (df(v) + 1) ]).\n",
    "\n",
    "Above the default is smooth:\n",
    "idf(t) = log [ (1 + D) / (1 + df(v)) ] + 1.\n",
    "\n",
    "But then the default setting normalizes by the eucledian norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2231435513142097"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "#Take the term donald:\n",
    "#a) mentioned once\n",
    "#b) D=4\n",
    "#c) df(v)=3\n",
    "1*(math.log(5/4)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.916290731874155"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take the term \"countries\" in the second document:\n",
    "#a) mentioned once\n",
    "#b) D=4\n",
    "#c) df(v)=1\n",
    "import math\n",
    "1*(math.log(5/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0216512475319814"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take the term \"president\" in the first document:\n",
    "#a) mentioned twice\n",
    "#b) D=4\n",
    "#c) df(v)=2\n",
    "import math\n",
    "2*(math.log(5/3)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So this is not downweighted so strongly and appears more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session4-wef-vectors (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
