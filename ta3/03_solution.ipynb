{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Three Bugs\n",
    "\n",
    "### Bug 2: Wrong column name\n",
    "\n",
    "The column is `date`, not `publication_date`. The error message says exactly this:\n",
    "\n",
    "```\n",
    "ColumnNotFoundError: publication_date\n",
    "```\n",
    "\n",
    "**Fix:** Change `publication_date` → `date`.\n",
    "\n",
    "### Bug 3: Wrong sort order\n",
    "\n",
    "\"Longest first\" means descending order. `sort(\"word_count\")` defaults to ascending.\n",
    "\n",
    "**Fix:** `.sort(\"word_count\", descending=True)`\n",
    "\n",
    "### Bug 1: `read_csv` instead of `scan_csv`\n",
    "\n",
    "`pl.read_csv()` loads the entire file into memory—exactly like pandas.\n",
    "\n",
    "`pl.scan_csv()` creates a **lazy frame**. No data is read until you call `.collect()`. This is the whole point of Polars for large files.\n",
    "\n",
    "**Fix:** Change `read_csv` → `scan_csv`, add `.collect()` at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51306 articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>title</th><th>text</th><th>url</th><th>domain</th><th>word_count</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;2023-02-03&quot;</td><td>&quot;Climate: rises government&quot;</td><td>&quot;President market be more of in…</td><td>&quot;https://washingtonpost.com/art…</td><td>&quot;washingtonpost.com&quot;</td><td>1481</td></tr><tr><td>&quot;2023-02-15&quot;</td><td>&quot;market announces World pressur…</td><td>&quot;Is the climate investment this…</td><td>&quot;https://nytimes.com/article/79…</td><td>&quot;nytimes.com&quot;</td><td>1481</td></tr><tr><td>&quot;2023-02-23&quot;</td><td>&quot;Breaking: government falls pre…</td><td>&quot;Climate market and by investme…</td><td>&quot;https://wsj.com/article/439100&quot;</td><td>&quot;wsj.com&quot;</td><td>1481</td></tr><tr><td>&quot;2023-02-13&quot;</td><td>&quot;Report: Economy shifts amid go…</td><td>&quot;Statement the policy report gr…</td><td>&quot;https://thehill.com/article/40…</td><td>&quot;thehill.com&quot;</td><td>1480</td></tr><tr><td>&quot;2023-02-01&quot;</td><td>&quot;industry faces Entertainment c…</td><td>&quot;Statement growth year billion …</td><td>&quot;https://axios.com/article/7219…</td><td>&quot;axios.com&quot;</td><td>1480</td></tr><tr><td>&quot;2023-02-06&quot;</td><td>&quot;Health: shifts officials&quot;</td><td>&quot;At president in government on …</td><td>&quot;https://chicagotribune.com/art…</td><td>&quot;chicagotribune.com&quot;</td><td>1480</td></tr><tr><td>&quot;2023-02-28&quot;</td><td>&quot;Breaking: officials changes un…</td><td>&quot;Growth president it it governm…</td><td>&quot;https://vice.com/article/15622…</td><td>&quot;vice.com&quot;</td><td>1480</td></tr><tr><td>&quot;2023-02-02&quot;</td><td>&quot;Breaking: government rises dec…</td><td>&quot;To announced been be policy co…</td><td>&quot;https://latimes.com/article/65…</td><td>&quot;latimes.com&quot;</td><td>1480</td></tr><tr><td>&quot;2023-02-23&quot;</td><td>&quot;sector announces Health growth&quot;</td><td>&quot;Statement investment the new h…</td><td>&quot;https://politico.com/article/3…</td><td>&quot;politico.com&quot;</td><td>1480</td></tr><tr><td>&quot;2023-02-27&quot;</td><td>&quot;Breaking: sector changes conce…</td><td>&quot;In technology policy economy i…</td><td>&quot;https://time.com/article/28830…</td><td>&quot;time.com&quot;</td><td>1480</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌────────────┬──────────────────┬─────────────────┬─────────────────┬─────────────────┬────────────┐\n",
       "│ date       ┆ title            ┆ text            ┆ url             ┆ domain          ┆ word_count │\n",
       "│ ---        ┆ ---              ┆ ---             ┆ ---             ┆ ---             ┆ ---        │\n",
       "│ str        ┆ str              ┆ str             ┆ str             ┆ str             ┆ u32        │\n",
       "╞════════════╪══════════════════╪═════════════════╪═════════════════╪═════════════════╪════════════╡\n",
       "│ 2023-02-03 ┆ Climate: rises   ┆ President       ┆ https://washing ┆ washingtonpost. ┆ 1481       │\n",
       "│            ┆ government       ┆ market be more  ┆ tonpost.com/art ┆ com             ┆            │\n",
       "│            ┆                  ┆ of in…          ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-15 ┆ market announces ┆ Is the climate  ┆ https://nytimes ┆ nytimes.com     ┆ 1481       │\n",
       "│            ┆ World pressur…   ┆ investment      ┆ .com/article/79 ┆                 ┆            │\n",
       "│            ┆                  ┆ this…           ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-23 ┆ Breaking:        ┆ Climate market  ┆ https://wsj.com ┆ wsj.com         ┆ 1481       │\n",
       "│            ┆ government falls ┆ and by          ┆ /article/439100 ┆                 ┆            │\n",
       "│            ┆ pre…             ┆ investme…       ┆                 ┆                 ┆            │\n",
       "│ 2023-02-13 ┆ Report: Economy  ┆ Statement the   ┆ https://thehill ┆ thehill.com     ┆ 1480       │\n",
       "│            ┆ shifts amid go…  ┆ policy report   ┆ .com/article/40 ┆                 ┆            │\n",
       "│            ┆                  ┆ gr…             ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-01 ┆ industry faces   ┆ Statement       ┆ https://axios.c ┆ axios.com       ┆ 1480       │\n",
       "│            ┆ Entertainment c… ┆ growth year     ┆ om/article/7219 ┆                 ┆            │\n",
       "│            ┆                  ┆ billion …       ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-06 ┆ Health: shifts   ┆ At president in ┆ https://chicago ┆ chicagotribune. ┆ 1480       │\n",
       "│            ┆ officials        ┆ government on … ┆ tribune.com/art ┆ com             ┆            │\n",
       "│            ┆                  ┆                 ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-28 ┆ Breaking:        ┆ Growth          ┆ https://vice.co ┆ vice.com        ┆ 1480       │\n",
       "│            ┆ officials        ┆ president it it ┆ m/article/15622 ┆                 ┆            │\n",
       "│            ┆ changes un…      ┆ governm…        ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-02 ┆ Breaking:        ┆ To announced    ┆ https://latimes ┆ latimes.com     ┆ 1480       │\n",
       "│            ┆ government rises ┆ been be policy  ┆ .com/article/65 ┆                 ┆            │\n",
       "│            ┆ dec…             ┆ co…             ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-23 ┆ sector announces ┆ Statement       ┆ https://politic ┆ politico.com    ┆ 1480       │\n",
       "│            ┆ Health growth    ┆ investment the  ┆ o.com/article/3 ┆                 ┆            │\n",
       "│            ┆                  ┆ new h…          ┆ …               ┆                 ┆            │\n",
       "│ 2023-02-27 ┆ Breaking: sector ┆ In technology   ┆ https://time.co ┆ time.com        ┆ 1480       │\n",
       "│            ┆ changes conce…   ┆ policy economy  ┆ m/article/28830 ┆                 ┆            │\n",
       "│            ┆                  ┆ i…              ┆ …               ┆                 ┆            │\n",
       "└────────────┴──────────────────┴─────────────────┴─────────────────┴─────────────────┴────────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE FIXED CODE, takes a minute to run\n",
    "import polars as pl\n",
    "\n",
    "result = (\n",
    "    pl.scan_csv(\"../data/cc_news_large.csv\")  # Bug 1 fixed: scan, not read\n",
    "    .filter(pl.col(\"date\") >= \"2023-02-01\")  # Bug 2 fixed: correct column name\n",
    "    .filter(pl.col(\"date\") < \"2023-03-01\")\n",
    "    .with_columns([\n",
    "        pl.col(\"text\").str.split(\" \").list.len().alias(\"word_count\")\n",
    "    ])\n",
    "    .sort(\"word_count\", descending=True)  # Bug 3 fixed: descending for \"longest first\"\n",
    "    .collect()  # Required when using scan_csv\n",
    ")\n",
    "\n",
    "print(f\"Found {len(result)} articles\")\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: DuckDB Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        domain,\n",
    "        COUNT(*) as election_article_count\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    WHERE date >= '2023-01-01' \n",
    "      AND date < '2024-01-01'\n",
    "      AND text ILIKE '%election%'\n",
    "    GROUP BY domain\n",
    "    ORDER BY election_article_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Query completed in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nTop 5 domains for 'election' articles in 2023:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key points about this query:\n",
    "\n",
    "1. **`ILIKE`** for case-insensitive matching (catches \"Election\", \"ELECTION\", etc.)\n",
    "2. **Date range** uses `>=` and `<` pattern (cleaner than BETWEEN for dates)\n",
    "3. **GROUP BY** before ORDER BY—aggregation first, then sorting\n",
    "4. The entire 10GB file was scanned but only matching rows were materialized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reasoning Answers\n",
    "\n",
    "### 3a: RAM and file size limits\n",
    "\n",
    "**Rule of thumb:** Pandas needs 2-5x the file size in RAM.\n",
    "\n",
    "If you have 8GB available RAM:\n",
    "- Conservative estimate: 8GB / 5 = **1.6GB max file**\n",
    "- Optimistic estimate: 8GB / 2 = **4GB max file**\n",
    "\n",
    "In practice, aim for files at most **20-30% of your available RAM** to leave room for other operations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3b: DuckDB vs Polars\n",
    "\n",
    "**Choose DuckDB when:**\n",
    "- You're doing one-off exploratory queries\n",
    "- You need to JOIN multiple large files (DuckDB's query optimizer shines here)\n",
    "- Your team knows SQL better than Python\n",
    "- You're working with Parquet files (native support, very fast)\n",
    "\n",
    "**Choose Polars when:**\n",
    "- You're building a data pipeline that will be reused\n",
    "- You need to do complex transformations that are awkward in SQL\n",
    "- You want to mix lazy and eager evaluation strategically\n",
    "- You're already in a Python codebase and want consistent API style\n",
    "\n",
    "---\n",
    "\n",
    "### 3c: 100GB file strategy\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "1. **Use DuckDB or Polars (lazy)** to filter to the target month first. This reduces 100GB → maybe 3GB (one month of ~36 months).\n",
    "\n",
    "2. **For word counting:** DuckDB can do this in SQL using `regexp_split_to_array` and `unnest`, but it's clunky. Better to:\n",
    "   - Filter with DuckDB/Polars\n",
    "   - Export filtered data to Parquet\n",
    "   - Process the smaller Parquet file (now RAM-feasible) for word frequency\n",
    "\n",
    "3. **Alternative:** Use DuckDB's string functions directly:\n",
    "   ```sql\n",
    "   SELECT word, COUNT(*) as freq\n",
    "   FROM (\n",
    "       SELECT UNNEST(string_split(text, ' ')) as word\n",
    "       FROM read_csv_auto('file.csv')\n",
    "       WHERE date >= '2023-03-01' AND date < '2023-04-01'\n",
    "   )\n",
    "   GROUP BY word\n",
    "   ORDER BY freq DESC\n",
    "   LIMIT 1000\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Common Mistakes Seen\n",
    "\n",
    "1. **Using `read_csv` instead of `scan_csv` in Polars**\n",
    "   - Symptom: Memory spike, slow execution\n",
    "   - Fix: Always use `scan_csv` for large files\n",
    "\n",
    "2. **Forgetting `.collect()` after Polars lazy operations**\n",
    "   - Symptom: Get a LazyFrame object, not data\n",
    "   - Fix: Add `.collect()` when you want actual results\n",
    "\n",
    "3. **Using `LIKE` instead of `ILIKE` in DuckDB**\n",
    "   - Symptom: Missing case variations (\"Election\" vs \"election\")\n",
    "   - Fix: Use `ILIKE` for case-insensitive search\n",
    "\n",
    "4. **Date filtering mistakes**\n",
    "   - Symptom: Getting January 1st of next year, or missing last day of month\n",
    "   - Fix: Use `>= start_date AND < next_month_first_day` pattern\n",
    "\n",
    "5. **Trying to load results that are still too big**\n",
    "   - Symptom: Filter works but `.df()` or `.collect()` crashes\n",
    "   - Fix: Add `LIMIT` or more filters; the result set is still too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Connecting to Remote PostgreSQL Database\n",
    "\n",
    "### Question 4: \"Cryptic task\"\n",
    "\n",
    "DuckDB has a PostgreSQL extension that allows querying remote databases directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "\n",
    "# Install and load the postgres extension\n",
    "conn = duckdb.connect()\n",
    "conn.execute(\"INSTALL postgres;\")\n",
    "conn.execute(\"LOAD postgres;\")\n",
    "\n",
    "# Connect to the remote PostgreSQL database\n",
    "# Format: postgresql://user:password@host:port/database\n",
    "postgres_conn_string = \"postgresql://user:BSEpass!$)@liip.econai.org:5432/BSE\"\n",
    "\n",
    "print(\"Connected to remote PostgreSQL database\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query the Remote Database\n",
    "\n",
    "First, let's find out what table exists in the synthetic schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exercise states \"the table is the only table in the schema\"\n",
    "# We need to discover the table name. Let's use ATTACH to query information_schema\n",
    "\n",
    "# Attach the PostgreSQL database to DuckDB\n",
    "conn.execute(f\"ATTACH '{postgres_conn_string}' AS postgres_db (TYPE POSTGRES, READ_ONLY);\")\n",
    "\n",
    "# Query information_schema to find tables in the synthetic schema\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM postgres_db.information_schema.tables\n",
    "    WHERE table_schema = 'synthetic'\n",
    "      AND table_type = 'BASE TABLE'\n",
    "\"\"\").fetchall()\n",
    "\n",
    "print(\"Tables in synthetic schema:\")\n",
    "for table in result:\n",
    "    print(f\"  - {table[0]}\")\n",
    "\n",
    "# Get the first (and only) table name\n",
    "table_name = result[0][0] if result else None\n",
    "print(f\"\\nUsing table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run a Sample Query and Compare Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now query using postgres_scan (3 arguments: connection, schema, table)\n",
    "print(\"Querying REMOTE PostgreSQL database...\")\n",
    "start_remote = time.time()\n",
    "\n",
    "remote_result = conn.execute(f\"\"\"\n",
    "    SELECT domain, COUNT(*) as article_count\n",
    "    FROM postgres_scan('{postgres_conn_string}', 'synthetic', '{table_name}')\n",
    "    WHERE date >= '2023-01-01' AND date < '2023-02-01'\n",
    "    GROUP BY domain\n",
    "    ORDER BY article_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "remote_time = time.time() - start_remote\n",
    "\n",
    "print(f\"Remote query completed in {remote_time:.2f} seconds\")\n",
    "print(\"\\nTop 10 domains (remote):\")\n",
    "print(remote_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the LOCAL CSV file with the same query\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Querying LOCAL CSV file...\")\n",
    "start_local = time.time()\n",
    "\n",
    "local_result = conn.execute(\"\"\"\n",
    "    SELECT domain, COUNT(*) as article_count\n",
    "    FROM read_csv_auto('../data/cc_news_large.csv')\n",
    "    WHERE date >= '2023-01-01' AND date < '2023-02-01'\n",
    "    GROUP BY domain\n",
    "    ORDER BY article_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "local_time = time.time() - start_local\n",
    "\n",
    "print(f\"Local query completed in {local_time:.2f} seconds\")\n",
    "print(\"\\nTop 10 domains (local):\")\n",
    "print(local_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TIMING COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Remote PostgreSQL: {remote_time:.2f} seconds\")\n",
    "print(f\"Local CSV:         {local_time:.2f} seconds\")\n",
    "print(f\"Difference:        {abs(remote_time - local_time):.2f} seconds\")\n",
    "print(f\"Ratio:             {remote_time/local_time:.2f}x\")\n",
    "\n",
    "if remote_time > local_time:\n",
    "    print(f\"\\nRemote was {remote_time/local_time:.2f}x SLOWER than local\")\n",
    "else:\n",
    "    print(f\"\\nRemote was {local_time/remote_time:.2f}x FASTER than local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Why is there a difference in times?\n",
    "\n",
    "**Key factors affecting remote database query performance:**\n",
    "\n",
    "#### Network Latency & Bandwidth\n",
    "- **Remote:** Data must travel over the internet from `liip.econai.org` to your machine\n",
    "- **Local:** Data is read from your local disk (or even RAM/cache)\n",
    "- Network latency adds 10-200ms per round trip, and large result sets are limited by bandwidth\n",
    "\n",
    "#### Server-Side Processing vs Client-Side\n",
    "- **Remote PostgreSQL:** The database server does filtering, aggregation, and sorting on its hardware\n",
    "- **Local CSV with DuckDB:** Your laptop CPU/RAM does all the processing\n",
    "- If the server has better hardware or the data is already indexed, remote can be faster\n",
    "\n",
    "#### Data Transfer Volume\n",
    "- **Smart query:** If you filter early and only transfer 10 rows, network overhead is minimal\n",
    "- **Large result set:** If you transfer millions of rows, bandwidth becomes the bottleneck\n",
    "- The `LIMIT 10` in our query means we only transfer 10 rows, reducing network impact\n",
    "\n",
    "#### Typical Observations:\n",
    "1. **Remote is SLOWER when:**\n",
    "   - Network latency is high\n",
    "   - Result sets are large (millions of rows to transfer)\n",
    "   - Server is under heavy load\n",
    "   - Query is simple (less benefit from server-side processing)\n",
    "\n",
    "2. **Remote can be FASTER when:**\n",
    "   - Data is pre-indexed on the server\n",
    "   - Server has much better hardware (more RAM, faster CPUs)\n",
    "   - Local disk I/O is slow\n",
    "   - Query is complex (server does heavy computation, returns small result)\n",
    "   - Multiple clients share the same data (no duplication)\n",
    "\n",
    "#### In This Exercise:\n",
    "The 16GB CSV file takes time to scan locally. If the PostgreSQL server has indexed the `date` column and has sufficient RAM to cache the table, it might actually be faster despite network overhead. However, for ad-hoc analytics on CSV-like data, local processing with DuckDB typically wins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
